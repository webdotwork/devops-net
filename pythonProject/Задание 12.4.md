# Домашнее задание к занятию "12.4 Развертывание кластера на собственных серверах, лекция 2"
Новые проекты пошли стабильным потоком. Каждый проект требует себе несколько кластеров: под тесты и продуктив. Делать все руками — не вариант, поэтому стоит автоматизировать подготовку новых кластеров.

## Задание 1: Подготовить инвентарь kubespray
Новые тестовые кластеры требуют типичных простых настроек. Нужно подготовить инвентарь и проверить его работу. Требования к инвентарю:
* подготовка работы кластера из 5 нод: 1 мастер и 4 рабочие ноды;
![image](https://user-images.githubusercontent.com/40559167/203134882-2c25c09c-1a41-40a5-810b-470a87e3c9e7.png)
* в качестве CRI — containerd;
```
www@www:~$ sudo kubectl get nodes -o wide
NAME    STATUS   ROLES           AGE    VERSION   INTERNAL-IP   EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION      CONTAINER-RUNTIME
node1   Ready    control-plane   6h7m   v1.25.4   10.130.0.32   <none>        Ubuntu 20.04.5 LTS   5.4.0-132-generic   containerd://1.6.10
node2   Ready    <none>          6h6m   v1.25.4   10.130.0.27   <none>        Ubuntu 20.04.5 LTS   5.4.0-132-generic   containerd://1.6.10
node3   Ready    <none>          6h6m   v1.25.4   10.130.0.6    <none>        Ubuntu 20.04.5 LTS   5.4.0-132-generic   containerd://1.6.10
node4   Ready    <none>          6h6m   v1.25.4   10.130.0.11   <none>        Ubuntu 20.04.5 LTS   5.4.0-132-generic   containerd://1.6.10
node5   Ready    <none>          6h6m   v1.25.4   10.130.0.10   <none>        Ubuntu 20.04.5 LTS   5.4.0-132-generic   containerd://1.6.10
```
```
www@www:~$ sudo kubectl get pods -o wide -A
NAMESPACE       NAME                                      READY   STATUS    RESTARTS       AGE     IP               NODE    NOMINATED NODE   READINESS GATES
ingress-nginx   ingress-nginx-controller-4glvl            1/1     Running   0              4h54m   10.233.97.130    node5   <none>           <none>
ingress-nginx   ingress-nginx-controller-b6s8j            1/1     Running   0              4h54m   10.233.74.65     node4   <none>           <none>
ingress-nginx   ingress-nginx-controller-bxf8w            1/1     Running   0              4h54m   10.233.75.2      node2   <none>           <none>
ingress-nginx   ingress-nginx-controller-n4q6b            1/1     Running   0              4h54m   10.233.71.1      node3   <none>           <none>
kube-system     calico-kube-controllers-d6484b75c-9wptv   1/1     Running   0              6h6m    10.233.75.0      node2   <none>           <none>
kube-system     calico-node-5t547                         1/1     Running   0              6h8m    10.130.0.11      node4   <none>           <none>
kube-system     calico-node-hs6fg                         1/1     Running   0              6h8m    10.130.0.27      node2   <none>           <none>
kube-system     calico-node-tfx9m                         1/1     Running   0              6h8m    10.130.0.32      node1   <none>           <none>
kube-system     calico-node-v5nbj                         1/1     Running   0              6h8m    10.130.0.6       node3   <none>           <none>
kube-system     calico-node-vclwt                         1/1     Running   0              6h8m    10.130.0.10      node5   <none>           <none>
kube-system     coredns-588bb58b94-8sdd2                  1/1     Running   0              6h6m    10.233.102.129   node1   <none>           <none>
kube-system     coredns-588bb58b94-trk7m                  1/1     Running   0              6h6m    10.233.97.129    node5   <none>           <none>
kube-system     dns-autoscaler-5b9959d7fc-92xcj           1/1     Running   0              6h6m    10.233.102.130   node1   <none>           <none>
kube-system     kube-apiserver-node1                      1/1     Running   2 (6h5m ago)   6h10m   10.130.0.32      node1   <none>           <none>
kube-system     kube-controller-manager-node1             1/1     Running   2 (6h5m ago)   6h10m   10.130.0.32      node1   <none>           <none>
kube-system     kube-proxy-2xq7p                          1/1     Running   0              4h56m   10.130.0.10      node5   <none>           <none>
kube-system     kube-proxy-4qbt4                          1/1     Running   0              4h56m   10.130.0.32      node1   <none>           <none>
kube-system     kube-proxy-78dnc                          1/1     Running   0              4h56m   10.130.0.11      node4   <none>           <none>
kube-system     kube-proxy-fwc8f                          1/1     Running   0              4h56m   10.130.0.6       node3   <none>           <none>
kube-system     kube-proxy-gwwrz                          1/1     Running   0              4h56m   10.130.0.27      node2   <none>           <none>
kube-system     kube-scheduler-node1                      1/1     Running   2 (6h5m ago)   6h10m   10.130.0.32      node1   <none>           <none>
kube-system     metrics-server-5c9dd56466-zwct2           1/1     Running   0              4h54m   10.233.102.131   node1   <none>           <none>
kube-system     nginx-proxy-node2                         1/1     Running   0              6h7m    10.130.0.27      node2   <none>           <none>
kube-system     nginx-proxy-node3                         1/1     Running   0              6h7m    10.130.0.6       node3   <none>           <none>
kube-system     nginx-proxy-node4                         1/1     Running   0              6h7m    10.130.0.11      node4   <none>           <none>
kube-system     nginx-proxy-node5                         1/1     Running   0              6h7m    10.130.0.10      node5   <none>           <none>
kube-system     nodelocaldns-5vf8x                        1/1     Running   0              6h6m    10.130.0.27      node2   <none>           <none>
kube-system     nodelocaldns-fcpkv                        1/1     Running   0              6h6m    10.130.0.32      node1   <none>           <none>
kube-system     nodelocaldns-l6r97                        1/1     Running   0              6h6m    10.130.0.10      node5   <none>           <none>
kube-system     nodelocaldns-vvkns                        1/1     Running   0              6h6m    10.130.0.6       node3   <none>           <none>
kube-system     nodelocaldns-wsg2k                        1/1     Running   0              6h6m    10.130.0.11      node4   <none>           <none>
```
* запуск etcd производить на мастере.
```
www@www:~/Desktop/CI/kb/kubespray/inventory/mycluster$ cat hosts.yaml
all:
  hosts:
    node1:
      ansible_host: 51.250.34.213
      ansible_user: yc-user
    node2:
      ansible_host: 51.250.38.255
      ansible_user: yc-user
    node3:
      ansible_host: 51.250.45.65
      ansible_user: yc-user
    node4:
      ansible_host: 51.250.41.49
      ansible_user: yc-user
    node5:
      ansible_host: 51.250.40.252
      ansible_user: yc-user
  children:
    kube_control_plane:
      hosts:
        node1:
    kube_node:
      hosts:
        node2:
        node3:
        node4:
        node5:
    etcd:
      hosts:
        node1:
    k8s_cluster:
      children:
        kube_control_plane:
        kube_node:
    calico_rr:
      hosts: {}
```
```
www@www:~/Desktop/CI/kb/kubespray/inventory/mycluster$ cat inventory.ini
# ## Configure 'ip' variable to bind kubernetes services on a
# ## different ip than the default iface
# ## We should set etcd_member_name for etcd cluster. The node that is not a etcd member do not need to set the value, or can set the empty string value.
[all]
# node1 ansible_host=51.250.34.213  # ip=10.130.0.32 etcd_member_name=etcd1
# node2 ansible_host=51.250.38.255  # ip=10.130.0.27 etcd_member_name=etcd2
# node3 ansible_host=51.250.45.65   # ip=10.130.0.6  etcd_member_name=etcd3
# node4 ansible_host=51.250.41.49   # ip=10.130.0.11 etcd_member_name=etcd4
# node5 ansible_host=51.250.40.252  # ip=10.130.0.10 etcd_member_name=etcd5


# ## configure a bastion host if your nodes are not directly reachable
# [bastion]
# bastion ansible_host=x.x.x.x ansible_user=some_user

[kube_control_plane]
# node1


[etcd]
# node1


[kube_node]
# node2
# node3
# node4
# node5

[calico_rr]

[k8s_cluster:children]
kube_control_plane
kube_node
calico_rr
```

## Задание 2 (*): подготовить и проверить инвентарь для кластера в AWS
Часть новых проектов хотят запускать на мощностях AWS. Требования похожи:
* разворачивать 5 нод: 1 мастер и 4 рабочие ноды;
* работать должны на минимально допустимых EC2 — t3.small.

---

### Как оформить ДЗ?

Выполненное домашнее задание пришлите ссылкой на .md-файл в вашем репозитории.

---
